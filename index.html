<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fanqi Wan's homepage</title>

  <meta name="author" content="Fanqi Wan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fanqi Wan (‰∏áÂá°Áê¶)</name>
              </p>
              <p>Hello! My name is Fanqi Wan, and I am a second-year MS student at Sun Yat-sen University, advised by <a href="https://sites.google.com/site/xiaojunquan/">Prof. Xiaojun Quan</a>. Before this, I received my Bachelor's degree from Xi'an Jiaotong University. Previously, I conducted my internship at Tencent AI Lab (2023.03-2024.05), where I am mentored by <a href="https://timhuang1.github.io/">Dr. Xinting Huang</a> and <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ&hl=zh-CN">Dr. Wei Bi</a>. Now, I am currently conducting my internship at ByteDance Doubao (Seed) LLM Team (2024.06-Now).
                </p>
              <p style="text-align:center">
                  <a href="mailto:fanqiwan2018@gmail.com">Email</a>  /
                  CV(<a href="images/Fanqi_Wan_s_CV_EN.pdf">en</a>/<a href="images/Fanqi_Wan_s_CV_ZH.pdf">zh</a>)  /
                  <a href="https://scholar.google.com/citations?user=AeS1tmEAAAAJ&hl=zh-CN">Google Scholar</a>  /
		  <a href="https://www.semanticscholar.org/author/Fanqi-Wan/2217614543">Semantic Scholar</a>  /    
                  <a href="https://github.com/fanqiwan">GitHub</a>  /
		  <a href="https://huggingface.co/Wanfq">HF</a>  /
                  <a href="https://twitter.com/fanqiwan">X</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/fanqiwan.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/fanqiwan.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My main research interests focused on deep learning for natural language generation. Previously, my work primarily focused on dialogue systems. After the emergence of large language models (LLMs), my research direction shifted towards efficient alignment (e.g., self-evolution for LLMs optimization, developing LLMs for specific domains, mitigating hallucinations of LLMs) and knowledge fusion (e.g., combining the strengths of LLMs with diverse architectures and scales). Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td colspan="2" style="padding:20px;text-align:left;font-weight:bold;background-color:#ffffff;">
        Knowledge Fusion
      </td>
    </tr>
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/fusechat.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2408.07990" id="conf_fusechat">
          <papertitle><span class="highlight">FuseChat: Knowledge Fusion of Chat Models</span></papertitle>
        </a>
        <br>
        <strong>Fanqi Wan</strong>,
	Longguang Zhong,
	Ziyi Yang,
	Ruijun Chen,
	Xiaojun Quan
        <br>
        <em>Tech Report</em>, 2024
        <br>
        <a href="https://github.com/fanqiwan/FuseLLM">[GitHub]</a>
        /
	<a href="https://huggingface.co/collections/Wanfq/fusechat-20-66bf3462c55655c7152e4e23">[HF]</a>
        /
        <a href="https://arxiv.org/abs/2408.07990">[Paper]</a>
	/
        <a href="https://twitter.com/_akhaliq/status/1762344073820578145">[Featured by AK]</a>
	/
	<a href="https://huggingface.co/papers?date=2024-02-27">[HF Daily Papers]</a>
	/
        <a href="https://mp.weixin.qq.com/s/wD9uP33jTZX5jINadcn8IQ">[Êú∫Âô®‰πãÂøÉ]</a>
        <p></p>
        <p>We propose FuseChat, an extended framework of FuseLLM to integrate the collective knowledge and individual strengths of multiple structure- and scale-varied chat LLMs into a more powerful chat LLM. FuseChat-7B is <strong>comparable to the larger Mixtral-8x7B-Instruct and and approaches GPT-3.5-Turbo-1106</strong> on MT-Bench.</p>
      </td>
    </tr>
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/fusellm.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2401.10491" id="conf_fusellm">
          <papertitle><span class="highlight">Knowledge Fusion of Large Language Models</span></papertitle>
        </a>
        <br>
        <strong>Fanqi Wan</strong>,
        Xinting Huang,
        Deng Cai,
        Xiaojun Quan,
        Wei Bi,
        Shuming Shi
        <br>
        <em>ICLR</em>, 2024
        <br>
        <a href="https://github.com/fanqiwan/FuseLLM">[GitHub]</a>
        /
	<a href="https://huggingface.co/collections/Wanfq/fusellm-65ad101d3022589371ff960e">[HF]</a>
        /
        <a href="https://arxiv.org/abs/2401.10491">[Paper]</a>
	/
        <a href="https://twitter.com/omarsar0/status/1749267663900057620">[Featured by elvis]</a>
	/
        <a href="https://twitter.com/ai_database/status/1749600484397363226">[Featured by AIDB]</a>
	/
        <a href="https://mp.weixin.qq.com/s/7JJBB_lNHKVoeaMtvCEyfQ">[Êú∫Âô®‰πãÂøÉ]</a>
        <p></p>
        <p>We propose FuseLLM to create a unified model that combines the distinctive strengths of multiple structurally diverse LLMs. FuseLLM-7B <strong>surpasses Llama-2-7B on 12 benchmarks</strong>, including commonsense, reasoning, question-answering, and code generation.</p>
      </td>
    </tr>

    <tr>
      <td colspan="2" style="padding:20px;text-align:left;font-weight:bold;background-color:#ffffff;">
        Efficient Alignment
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/seft.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2406.10813" id="conf_seft">
          <papertitle>Self-Evolution Fine-Tuning for Policy Optimization</papertitle>
        </a>
        <br>
	Ruijun Chen, 
	Jiehao Liang, 
	Shiping Gao,
        <strong>Fanqi Wan</strong>,
        Xiaojun Quan
        <br>
        <em>Tech Report</em>, 2024
        <br>
        <a href="https://github.com/xxxx/xxxx">[GitHub]</a>
	/
        <a href="https://arxiv.org/abs/2406.10813">[Paper]</a>
        <p></p>
        <p>We introduce self-evolution fine-tuning for policy optimization, which eliminates the need for annotated data samples during alignment.</p>
      </td>
    </tr>
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/kca.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2401.10768" id="conf_kca">
          <papertitle><span class="highlight">Knowledge Verification to Nip Hallucination in the Bud</span></papertitle>
        </a>
        <br>
        <strong>Fanqi Wan</strong>,
        Xinting Huang,
        Leyang Cui,
        Xiaojun Quan,
        Wei Bi,
        Shuming Shi
        <br>
        <em>Tech Report</em>, 2024
        <br>
        <a href="https://github.com/fanqiwan/KCA">[GitHub]</a>
	/
	<a href="https://huggingface.co/collections/Wanfq/kca-65aa9ab25c84d7d5e7c9d0da">[HF]</a>
        /
        <a href="https://arxiv.org/abs/2401.10768">[Paper]</a>
        <p></p>
        <p>We introduce Knowledge Consistent Alignment to <strong>verify and minimize the knowledge inconsistency</strong> between external knowledge in the alignment data and the intrinsic knowledge embedded in foundation LLMs, thus mitigating hallucinations before alignment.</p>
      </td>
    </tr>
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/explore_instruct.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2310.09168" id="conf_explore-instruct">
          <papertitle><span class="highlight">Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration</span></papertitle>
        </a>
        <br>
        <strong>Fanqi Wan</strong>,
        Xinting Huang,
        Tao Yang,
        Xiaojun Quan,
        Wei Bi,
        Shuming Shi
        <br>
        <em>EMNLP</em>, 2023
        <br>
        <a href="https://github.com/fanqiwan/Explore-Instruct">[GitHub]</a>
	/
	<a href="https://huggingface.co/collections/Wanfq/explore-instruct-65280973f74b8bf3e9f9bd7e">[HF]</a>
        /
        <a href="https://arxiv.org/abs/2310.09168">[Paper]</a>
	/
        <a href="https://mp.weixin.qq.com/s/rDw8avX2pfe2T9xBDs9NGQ">[Paper Weekly]</a>
        <p></p>
        <p>We propose a novel approach to enhance the domain-specific instruction coverage by utilizing LLMs to explore the domain space from both breadth and depth automatically. Explore-Instruct <strong>outperforms Self-Instruct in three specific domains</strong>.</p>
      </td>
    </tr>

    <tr>
      <td colspan="2" style="padding:20px;text-align:left;font-weight:bold;background-color:#ffffff;">
        Dialogue Systems
      </td>
    </tr>
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/mk_tod.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2310.08877" id="conf_mk-tod">
          <papertitle>Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System</papertitle>
        </a>
        <br>
        Weizhou Shen,
        Yingqi Gao,
        Canbin Huang,
        <strong>Fanqi Wan</strong>,
        Xiaojun Quan,
        Wei Bi
        <br>
        <em>EMNLP</em>, 2023
        <br>
        <a href="https://github.com/shenwzh3/MK-TOD">[GitHub]</a>
        /
        <a href="https://arxiv.org/abs/2310.08877">[Paper]</a>
        <p></p>
        <p>We introduce maximal marginal likelihood for retriever training to address the retrieval-generation misalignment in end-to-end task-oriented dialogue systems.</p>
      </td>    
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/maker.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2305.10149" id="conf_maker">
          <papertitle><span class="highlight">Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog</span></papertitle>
        </a>
        <br>
        <strong>Fanqi Wan</strong>,
        Weizhou Shen,
        Ke Yang,
        Xiaojun Quan,
        Wei Bi
        <br>
        <em>ACL</em>, 2023
        <br>
        <a href="https://github.com/18907305772/MAKER">[GitHub]</a>
	/
	<a href="https://huggingface.co/collections/Wanfq/maker-6527fde2a4be83259cdfc9e9">[HF]</a>
        /
        <a href="https://arxiv.org/abs/2305.10149">[Paper]</a>
        <p></p>
        <p>We propose a multi-grained knowledge retriever and introduce a novel distillation objective for retriever training. MAKER achieves <strong>SOTA performance on MultiWOZ 2.1 and CamRest</strong> with both condensed KB and full KB.</p>
      </td>    
    </tr>

    <tr>
      <td colspan="2" style="padding:20px;text-align:left;font-weight:bold;background-color:#ffffff;">
        Misc.
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/blockpruner.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2406.10594" id="conf_blockpruner">
          <papertitle>BlockPruner: Fine-grained Pruning for Large Language Models</papertitle>
        </a>
        <br>
	Longguang Zhong,
        <strong>Fanqi Wan</strong>,
	Ruijun Chen,
        Xiaojun Quan,
	Liangzhi Li
        <br>
        <em>Tech Report</em>, 2024
        <br>
        <a href="https://github.com/MrGGLS/BlockPruner">[GitHub]</a>
        /
        <a href="https://arxiv.org/abs/2406.10594">[Paper]</a>
        <p></p>
        <p>We propose a training-free structured pruning approach for fine-grained LLMs pruning.</p>
      </td>    
    </tr>    
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/psycot.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2310.20256" id="conf_psycot">
          <papertitle>PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection</papertitle>
        </a>
        <br>
        Tao Yang,
        Tianyuan Shi,
        <strong>Fanqi Wan</strong>,
        Xiaojun Quan,
        Qifan Wang,
        Bingzhe Wu,
        Jiaxiang Wu
        <br>
        <em>EMNLP Findings</em>, 2023
        <br>
        <a href="https://github.com/TaoYang225/PsyCoT">[GitHub]</a>
        /
        <a href="https://arxiv.org/abs/2310.20256">[Paper]</a>
        <p></p>
        <p>We propose a novel method for zero-shot personality detection in a multi-turn dialogue manner.</p>
      </td>    
    </tr>
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/clusterns.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2305.09892" id="conf_clusterns">
          <papertitle>Clustering-Aware Negative Sampling for Unsupervised Sentence Representation</papertitle>
        </a>
        <br>
        Jinghao Deng,
        <strong>Fanqi Wan</strong>,
        Tao Yang,
        Xiaojun Quan,
        Rui Wang
        <br>
        <em>ACL Findings</em>, 2023
        <br>
        <a href="https://github.com/djz233/clusterns">[GitHub]</a>
        /
        <a href="https://arxiv.org/abs/2305.09892">[Paper]</a>
        <p></p>
        <p>We propose a novel method that incorporates cluster information into contrastive learning for unsupervised sentence representation learning.</p>
      </td>
    </tr>


    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Education</heading>
        <p>
          MS Student in Computer Science, <strong>Sun Yat-sen University</strong> (2022.9-now).
        </p>
        <p>
          Bachelor of Automation, <strong>Xi'an Jiaotong University</strong> (2018.9-2022.6).
        </p>
      </td>
    </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Experience</heading>
        <p>
          Research Intern at LLM Team, <strong>ByteDance Doubao (Seed)</strong>. (2024.6-now).
        </p>
	<p>
          Research Intern at NLPC, <strong>Tencent AI Lab</strong>, supervised by Dr. Xinting Huang and Dr. Wei Bi. (2023.3-2024.5).
        </p>
	<p>
          Commercial Projects on E-Commerce Platforms, <strong>Vipshop</strong>, supervised by Dr. Rui Wang. (2022.4-2023.1).
        </p>
      </td>
    </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Academic Competitions</heading>
        <p>
          <strong>2nd Prize</strong> on 2023 Xingzhi Cup Deep Learning Model Interpretability Task. (<strong>Team Leader</strong>) <a href="http://www.aiinnovation.com.cn/#/trackDetail?id=23">[Task]</a>
        </p>
        <p>
          <strong>2nd Prize</strong> on 2022 IFLYTEK AI Developer Competition Paper Abstracts Classification Task. (<strong>Team Leader</strong>) <a href="https://challenge.xfyun.cn/topic/info?type=abstract&option=phb">[Task]</a>
        </p>
        <p>
          <strong>3nd Prize</strong> on 2022 Ali Lingjie E-commerce Search Algorithm Competition. (<strong>Team Leader</strong>) <a href="https://tianchi.aliyun.com/specials/promotion/opensearch">[Task]</a>
        </p>
      </td>
    </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Selected Awards</heading>
	<p>
          <strong>Outstanding Award for Tencent AI Lab Rhino-Bird Focused Research Program</strong>, Tencent, 2023.
        </p>
        <p>
          <strong>Excellent Graduate Students</strong>, Xi'an Jiaotong University, 2022.
        </p>
        <p>
          <strong>National Scholarship</strong>, Xi'an Jiaotong University, 2019.
        </p>
      </td>
    </tr>
    </tbody></table>

<!--  </tbody></table>-->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          Website's code is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
          
        </p>
      </td>
    </tr>
  </tbody></table>
</td>
</tr>
  </tbody></table>
</body>

</html>
